{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9720916,"sourceType":"datasetVersion","datasetId":5947504}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Libraries","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport re\n\nfrom os.path import exists\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom scipy.sparse import hstack\n\nsns.set(style='whitegrid')","metadata":{"execution":{"iopub.status.busy":"2024-10-26T17:09:47.611364Z","iopub.execute_input":"2024-10-26T17:09:47.611713Z","iopub.status.idle":"2024-10-26T17:09:48.985706Z","shell.execute_reply.started":"2024-10-26T17:09:47.611665Z","shell.execute_reply":"2024-10-26T17:09:48.984888Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Loading the Files\n\nDownload the csv files into the `data/` directory.","metadata":{}},{"cell_type":"code","source":"trainingSet = pd.read_csv(\"../input/dataset/train.csv\")\ntestingSet = pd.read_csv(\"../input/dataset/test.csv\")\n\n#print(\"train.csv shape is \", trainingSet.shape)\n#print(\"test.csv shape is \", testingSet.shape)\n\n#print()\n\n#print(trainingSet.head())\n#print()\n#print(testingSet.head())\n\n#print()\n\n#print(trainingSet.describe())\n\n#trainingSet['Score'].value_counts().plot(kind='bar', legend=True, alpha=.5)\n#plt.show()\n\n#print()\n#print(\"EVERYTHING IS PROPERLY SET UP! YOU ARE READY TO START\")","metadata":{"execution":{"iopub.status.busy":"2024-10-26T17:09:48.986732Z","iopub.execute_input":"2024-10-26T17:09:48.987188Z","iopub.status.idle":"2024-10-26T17:10:19.512325Z","shell.execute_reply.started":"2024-10-26T17:09:48.987155Z","shell.execute_reply":"2024-10-26T17:10:19.511267Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Sentiment Score Calculation\nTo capture the sentiment of each review, I created a simple sentiment score. This score is based on counting occurrences of predefined positive and negative words in the review text (the top 10 for each category were just words that I saw most frequently). For each review, the number of positive words is subtracted by the number of negative words. The goal was to assign a numerical value indicating whether the review leans more positively or negatively.","metadata":{}},{"cell_type":"code","source":"# Define the sentiment_score function separately\ndef sentiment_score(text):\n    positive_words = ['good', 'great', 'excellent', 'amazing', 'love', 'liked', 'wonderful', 'best', 'fantastic', 'enjoyed']\n    negative_words = ['bad', 'terrible', 'awful', 'hate', 'disliked', 'poor', 'worst', 'boring', 'disappointing', 'waste']\n    text = str(text).lower()\n    pos_count = sum([text.count(word) for word in positive_words])\n    neg_count = sum([text.count(word) for word in negative_words])\n    return pos_count - neg_count","metadata":{"execution":{"iopub.status.busy":"2024-10-26T17:10:19.513568Z","iopub.execute_input":"2024-10-26T17:10:19.513893Z","iopub.status.idle":"2024-10-26T17:10:19.522156Z","shell.execute_reply.started":"2024-10-26T17:10:19.513859Z","shell.execute_reply":"2024-10-26T17:10:19.521131Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Text Preprocessing\nBefore performing feature extraction, I applied basic preprocessing to clean up the review text.\nThis helps eliminate irrelevant characters and standardizes the text, improving the effectiveness of TF-IDF feature extraction.","metadata":{}},{"cell_type":"code","source":"# Define a function to preprocess text\ndef preprocess_text(text):\n    text = str(text).lower()\n    text = re.sub(r'http\\S+', '', text)  # Remove URLs\n    text = re.sub(r'<.*?>', '', text)    # Remove HTML tags\n    text = re.sub(r'[^a-zA-Z\\s]', '', text)  # Remove non-alphabetic characters\n    text = re.sub(r'\\s+', ' ', text).strip()  # Remove extra whitespace\n    return text","metadata":{"execution":{"iopub.status.busy":"2024-10-26T17:10:19.524840Z","iopub.execute_input":"2024-10-26T17:10:19.525166Z","iopub.status.idle":"2024-10-26T17:10:19.532811Z","shell.execute_reply.started":"2024-10-26T17:10:19.525133Z","shell.execute_reply":"2024-10-26T17:10:19.531876Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Adding Features\nThe add_features_to function is designed to transform the original dataset into a more refined and informative version. The goal of feature engineering is to extract meaningful patterns from the raw data, providing the model with relevant information for better prediction accuracy.\n","metadata":{}},{"cell_type":"code","source":"def add_features_to(df):\n    # This is where you can do all your feature extraction\n\n    # Handle division by zero and fill NaN values\n    df['Helpfulness'] = df['HelpfulnessNumerator'] / df['HelpfulnessDenominator']\n    df['Helpfulness'] = df['Helpfulness'].fillna(0)\n\n    # Cap extreme values to reduce impact of outliers\n    df['HelpfulnessNumerator'] = df['HelpfulnessNumerator'].clip(upper=100)\n    df['HelpfulnessDenominator'] = df['HelpfulnessDenominator'].clip(upper=100)\n\n    # Calculate the length of the review text and summary\n    df['ReviewLength'] = df['Text'].apply(lambda x: len(str(x).split()))\n    df['SummaryLength'] = df['Summary'].apply(lambda x: len(str(x).split()))\n\n    df['SentimentScore'] = df['Summary'].apply(sentiment_score) + df['Text'].apply(sentiment_score)\n\n    # Convert 'Time' to datetime and extract year and month\n    df['Time'] = pd.to_datetime(df['Time'], unit='s')\n    df['ReviewYear'] = df['Time'].dt.year\n    df['ReviewMonth'] = df['Time'].dt.month\n\n    # Combine 'Summary' and 'Text' into a single field\n    df['CombinedText'] = df['Summary'].astype(str) + ' ' + df['Text'].astype(str)\n\n    # Preprocess the combined text\n    df['CleanedText'] = df['CombinedText'].apply(preprocess_text)\n\n    return df\n\n# Load the feature extracted files if they've already been generated\nif exists('/kaggle/working/X_train.csv'):\n    X_train = pd.read_csv(\"/kaggle/working/X_train.csv\")\nif exists('/kaggle/working/X_submission.csv'):\n    X_submission = pd.read_csv(\"/kaggle/working/X_submission.csv\")\n\nelse:\n    # Process the DataFrame\n    train = add_features_to(trainingSet)\n\n    # Merge on Id so that the submission set can have feature columns as well\n    X_submission = pd.merge(train, testingSet, left_on='Id', right_on='Id')\n    X_submission = X_submission.drop(columns=['Score_x'])\n    X_submission = X_submission.rename(columns={'Score_y': 'Score'})\n\n    # The training set is where the score is not null\n    X_train =  train[train['Score'].notnull()]\n\n    X_submission.to_csv(\"/kaggle/working/X_submission.csv\", index=False)\n    X_train.to_csv(\"/kaggle/working/X_train.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2024-10-26T17:10:19.534032Z","iopub.execute_input":"2024-10-26T17:10:19.534422Z","iopub.status.idle":"2024-10-26T17:18:01.806375Z","shell.execute_reply.started":"2024-10-26T17:10:19.534388Z","shell.execute_reply":"2024-10-26T17:18:01.805491Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Sample + Split into training and testing set","metadata":{}},{"cell_type":"code","source":"# Split the training set into training and testing sets\nX = X_train.drop(columns=['Score'])\ny = X_train['Score']\n\nX_train_split, X_test_split, Y_train_split, Y_test_split = train_test_split(\n    X,\n    y,\n    test_size=1/4.0,\n    random_state=0\n)","metadata":{"execution":{"iopub.status.busy":"2024-10-26T17:18:01.807564Z","iopub.execute_input":"2024-10-26T17:18:01.807885Z","iopub.status.idle":"2024-10-26T17:18:03.395402Z","shell.execute_reply.started":"2024-10-26T17:18:01.807853Z","shell.execute_reply":"2024-10-26T17:18:03.394508Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# TF-IDF Vectorization\nIn this section, we use the TF-IDF technique to convert text data into a numerical format that machine learning models can process. We handle missing values in the 'CleanedText' columns by filling them with empty strings to avoid errors during vectorization. TF-IDF assigns weights to words based on their importance, helping the model focus on relevant terms while ignoring common words. We limit the number of features to 5,000 to capture the most frequent and meaningful words while keeping the model size manageable.","metadata":{}},{"cell_type":"code","source":"# Ensure 'CleanedText' does not contain NaN values\nX_train_split['CleanedText'] = X_train_split['CleanedText'].fillna('')\nX_test_split['CleanedText'] = X_test_split['CleanedText'].fillna('')\nX_submission['CleanedText'] = X_submission['CleanedText'].fillna('')\n\n# Initialize TF-IDF Vectorizer\ntfidf = TfidfVectorizer(max_features=5000, stop_words='english')\n\n# Fit and transform on training data\ntfidf_train = tfidf.fit_transform(X_train_split['CleanedText'])\n\n# Transform validation and submission data\ntfidf_test = tfidf.transform(X_test_split['CleanedText'])\ntfidf_submission = tfidf.transform(X_submission['CleanedText'])","metadata":{"execution":{"iopub.status.busy":"2024-10-26T17:18:03.396841Z","iopub.execute_input":"2024-10-26T17:18:03.397244Z","iopub.status.idle":"2024-10-26T17:22:28.596910Z","shell.execute_reply.started":"2024-10-26T17:18:03.397211Z","shell.execute_reply":"2024-10-26T17:22:28.595824Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Feature Selection\n\nIn this section, we select specific numeric features that provide additional context and help our model make better predictions. These features are derived from the text and metadata of each review and include aspects such as helpfulness scores, review lengths, sentiment scores, and temporal information.\n\nWe then prepare the numerical feature sets for each dataset (training, testing, and submission) by filling any missing values with zeros. This step ensures that all datasets are aligned and consistent for modeling.","metadata":{}},{"cell_type":"code","source":"# Select features to include in the model\nnumeric_features = [\n    'HelpfulnessNumerator', 'HelpfulnessDenominator', 'Helpfulness',\n    'ReviewLength', 'SummaryLength', 'SentimentScore',\n    'ReviewYear', 'ReviewMonth'\n]\n\n# Prepare numerical feature sets\nX_train_numeric = X_train_split[numeric_features].fillna(0)\nX_test_numeric = X_test_split[numeric_features].fillna(0)\nX_submission_numeric = X_submission[numeric_features].fillna(0)","metadata":{"execution":{"iopub.status.busy":"2024-10-26T17:22:28.598253Z","iopub.execute_input":"2024-10-26T17:22:28.598648Z","iopub.status.idle":"2024-10-26T17:22:28.698280Z","shell.execute_reply.started":"2024-10-26T17:22:28.598606Z","shell.execute_reply":"2024-10-26T17:22:28.697261Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Apply StandardScaler","metadata":{}},{"cell_type":"code","source":"# Standardize numerical features\nscaler = StandardScaler()\nX_train_numeric_scaled = scaler.fit_transform(X_train_numeric)\nX_test_numeric_scaled = scaler.transform(X_test_numeric)\nX_submission_numeric_scaled = scaler.transform(X_submission_numeric)\n\n# Combine TF-IDF features and numerical features\nX_train_combined = hstack([tfidf_train, X_train_numeric_scaled])\nX_test_combined = hstack([tfidf_test, X_test_numeric_scaled])\nX_submission_combined = hstack([tfidf_submission, X_submission_numeric_scaled])","metadata":{"execution":{"iopub.status.busy":"2024-10-26T17:22:28.699480Z","iopub.execute_input":"2024-10-26T17:22:28.699792Z","iopub.status.idle":"2024-10-26T17:22:30.663466Z","shell.execute_reply.started":"2024-10-26T17:22:28.699759Z","shell.execute_reply":"2024-10-26T17:22:30.662277Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Creation","metadata":{}},{"cell_type":"code","source":"# Initialize and train the Logistic Regression\nmodel = LogisticRegression(random_state=0, max_iter=200, solver='saga', n_jobs=-1)\nmodel.fit(X_train_combined, Y_train_split)\n\n# Predict on the test set\nY_test_predictions = model.predict(X_test_combined)","metadata":{"execution":{"iopub.status.busy":"2024-10-26T17:22:30.665033Z","iopub.execute_input":"2024-10-26T17:22:30.665357Z","iopub.status.idle":"2024-10-26T18:12:42.761806Z","shell.execute_reply.started":"2024-10-26T17:22:30.665324Z","shell.execute_reply":"2024-10-26T18:12:42.760988Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Evaluation","metadata":{}},{"cell_type":"code","source":"# Evaluate your model on the testing set\nprint(\"Accuracy on testing set = \", accuracy_score(Y_test_split, Y_test_predictions))\n\n# Plot a confusion matrix\ncm = confusion_matrix(Y_test_split, Y_test_predictions, normalize='true')\nsns.heatmap(cm, annot=True)\nplt.title('Confusion matrix of the classifier')\nplt.xlabel('Predicted')\nplt.ylabel('True')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-10-26T18:12:42.762992Z","iopub.execute_input":"2024-10-26T18:12:42.763333Z","iopub.status.idle":"2024-10-26T18:12:43.476446Z","shell.execute_reply.started":"2024-10-26T18:12:42.763299Z","shell.execute_reply":"2024-10-26T18:12:43.475548Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Create submission file","metadata":{}},{"cell_type":"code","source":"# Create the submission file\nX_submission['Score'] = model.predict(X_submission_combined)\nsubmission = X_submission[['Id', 'Score']]\nsubmission.to_csv(\"/kaggle/working/submission.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2024-10-26T18:12:43.477586Z","iopub.execute_input":"2024-10-26T18:12:43.477884Z","iopub.status.idle":"2024-10-26T18:12:44.502117Z","shell.execute_reply.started":"2024-10-26T18:12:43.477852Z","shell.execute_reply":"2024-10-26T18:12:44.501124Z"},"trusted":true},"execution_count":null,"outputs":[]}]}